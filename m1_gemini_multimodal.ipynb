{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9",
    "tags": []
   },
   "source": [
    "# Gemini: An Overview of Multimodal Use Cases\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fintro_multimodal_use_cases.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, you will explore a variety of different use cases enabled by multimodality with Gemini 1.0 Pro Vision.\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision and Gemini 1.0 Pro models.\n",
    "\n",
    "### Vertex AI Gemini API\n",
    "\n",
    "The Vertex AI Gemini API provides a unified interface for interacting with Gemini models. There are currently two models available in the Gemini API:\n",
    "\n",
    "- **Gemini 1.0 Pro model** (`gemini-1.0-pro`): Designed to handle natural language tasks, multiturn text and code chat, and code generation.\n",
    "- **Gemini 1.0 Pro Vision model** (`gemini-1.0-pro-vision`): Supports multimodal prompts. You can include text, images, and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more information, see the [Generative AI on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "This notebook demonstrates a variety of multimodal use cases that Gemini can be used for.\n",
    "\n",
    "#### Multimodal use cases\n",
    "\n",
    "Compared to text-only LLMs, Gemini 1.0 Pro Vision's multimodality can be used for many new use-cases:\n",
    "\n",
    "Example use cases with **text and image(s)** as input:\n",
    "\n",
    "- Detecting objects in photos\n",
    "- Understanding screens and interfaces\n",
    "- Understanding of drawing and abstraction\n",
    "- Understanding charts and diagrams\n",
    "- Recommendation of images based on user preferences\n",
    "- Comparing images for similarities, anomalies, or differences\n",
    "\n",
    "Example use cases with **text and video** as input:\n",
    "\n",
    "- Generating a video description\n",
    "- Extracting tags of objects throughout a video\n",
    "- Extracting highlights/messaging of a video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhsUe0fyc-ER"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRvKdaPDTznN",
    "outputId": "154a71b5-f302-4f53-ed2f-b3e5fef9195b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and intialize Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "GCP_PROJECT = PROJECT_ID\n",
    "LOCATION = REGION = 'asia-southeast1'\n",
    "\n",
    "print(f\"Project Name:\", GCP_PROJECT)\n",
    "print(f\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7rZuTClfNs0"
   },
   "source": [
    "## Use the Gemini 1.0 Pro Vision model\n",
    "\n",
    "Gemini 1.0 Pro Vision (`gemini-1.0-pro-vision`) is a multimodal model that supports multimodal prompts. You can include text, image(s), and video in your prompt requests and get text or code responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTNnM-lqfQRo"
   },
   "source": [
    "### Load Gemini 1.0 Pro Vision model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpL3OkSCfIAR"
   },
   "source": [
    "### Define helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7QMAHXse339",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_images([content])\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OWurhO4mu4J"
   },
   "source": [
    "## Image understanding across multiple images\n",
    "\n",
    "One of the capabilities of Gemini is being able to reason across multiple images.\n",
    "\n",
    "This is an  example of using Gemini to calculate the total cost of groceries using an image of fruits and a price list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "UyRoVquPmy9H",
    "outputId": "79fc24c3-6e92-4ae0-cf2e-53be0fb5577d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_grocery_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/banana-apple.jpg\"\n",
    "# Images to replace with\n",
    "# https://thumbs.dreamstime.com/b/fruit-basket-6262673.jpg\n",
    "# https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/banana-apple.jpg\n",
    "\n",
    "image_prices_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pricelist.jpg\"\n",
    "image_grocery = load_image_from_url(image_grocery_url)\n",
    "image_prices = load_image_from_url(image_prices_url)\n",
    "\n",
    "instructions = \"Instructions: Consider the following image that contains fruits:\"\n",
    "prompt1 = \"How much should I pay for the fruits given the following price list?\"\n",
    "prompt2 = \"\"\"\n",
    "Answer the question through these steps:\n",
    "Step 1: Identify what kind of fruits there are in the first image.\n",
    "Step 2: Count the quantity of each fruit.\n",
    "Step 3: For each grocery in first image, check the price of the grocery in the price list.\n",
    "Step 4: Calculate the subtotal price for each type of fruit.\n",
    "Step 5: Calculate the total price of fruits using the subtotals.\n",
    "\n",
    "Answer and describe the steps taken:\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    instructions,\n",
    "    image_grocery,\n",
    "    prompt1,\n",
    "    image_prices,\n",
    "    prompt2,\n",
    "]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy-me3PdgMUH",
    "tags": []
   },
   "source": [
    "## Understanding Screens and Interfaces\n",
    "\n",
    "Gemini can also extract information from appliance screens, UIs, screenshots, icons, and layouts.\n",
    "\n",
    "For example, if you input an image of a stove, you can ask Gemini to provide instructions to help a user navigate the UI and respond in different languages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDjN4thV8orx",
    "outputId": "9191a4ef-4dcc-4060-999d-c5425a986e1b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_stove_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\"\n",
    "# Images to replace with\n",
    "# https://res.cloudinary.com/sharkninja-na/image/upload/c_fit,h_438,w_584/v1/SharkNinja-NA/AF101C_02?_a=BAKAACDX0\n",
    "# https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\n",
    "\n",
    "image_stove = load_image_from_url(image_stove_url)\n",
    "\n",
    "prompt = \"\"\"How can I reset the clock on this appliance?\n",
    "Provide the instructions in English and then in French.\n",
    "If instructions include buttons, also explain where those buttons are physically located.\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "contents = [image_stove, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbhfMO478orx"
   },
   "source": [
    "Note: The response may not be completely accurate, as the model may hallucinate; however, the model is able to identify the location of buttons and translate in a single query. To mitigate hallucinations, one approach is to ground the LLM with retrieval-augmented generation, which is outside the scope of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "## Understanding entity relationships in technical diagrams\n",
    "\n",
    "Gemini has multimodal capabilities that enable it to understand diagrams and take actionable steps, such as optimization or code generation. This example demonstrates how Gemini can decipher an entity relationship (ER) diagram, understand the relationships between tables, identify requirements for optimization in a specific environment like BigQuery, and even generate corresponding code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klY4yBEiKmET",
    "outputId": "82320aea-57c4-4558-e267-9b76dcb13273",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_er_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/er.png\"\n",
    "image_er = load_image_from_url(image_er_url)\n",
    "\n",
    "prompt = \"Document the entities and relationships in this ER diagram.\"\n",
    "\n",
    "contents = [prompt, image_er]\n",
    "\n",
    "# Use a more deterministic configuration with a low temperature\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXrvQxpiQKp6"
   },
   "source": [
    "## Recommendations based on multiple images\n",
    "\n",
    "Gemini is capable of image comparison and providing recommendations. This may be useful in industries like e-commerce and retail.\n",
    "\n",
    "Below is an example of choosing which pair of glasses would be better suited to an oval-shaped face:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7M7B9q7L1X7",
    "outputId": "a33ed40b-a2a1-4dcc-908f-5b70cd60af0b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
    "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
    "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
    "image_glasses2 = load_image_from_url(image_glasses2_url)\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Which of these glasses you recommend for me based on the shape of my face?\n",
    "I have an oval shape face.\n",
    "----\n",
    "Glasses 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "----\n",
    "Glasses 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "----\n",
    "Explain how you reach out to this decision.\n",
    "Provide your recommendation based on my face shape, and reasoning for each in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt1, image_glasses1, prompt2, image_glasses2, prompt3]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN8nVlITK5kz",
    "tags": []
   },
   "source": [
    "## Generating a video description\n",
    "\n",
    "Gemini can also extract tags throughout a video:\n",
    "\n",
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tT2nArvxZv-P",
    "outputId": "81383d32-7f43-47d6-c375-d93cc5be5f4d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is shown in this video?\n",
    "Where should I go to see it?\n",
    "What are the top 5 places in the world that look like this?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBrdsvIU7Zkf",
    "tags": []
   },
   "source": [
    "## Similarity/Differences\n",
    "\n",
    "Gemini can compare images and identify similarities or differences between objects.\n",
    "\n",
    "The following example shows two scenes from Marienplatz in Munich, Germany that are slightly different. Gemini can compare between the images and find similarities/differences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUSJduLh8457",
    "outputId": "1743bf58-0717-44a6-cdb4-3f8f9e4c1093",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_landmark1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark1.jpg\"\n",
    "image_landmark2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark2.jpg\"\n",
    "image_landmark1 = load_image_from_url(image_landmark1_url)\n",
    "image_landmark2 = load_image_from_url(image_landmark2_url)\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Consider the following two images:\n",
    "Image 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "Image 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "1. What is shown in Image 1?\n",
    "2. What is similar between the two images?\n",
    "3. What is difference between Image 1 and Image 2 in terms of the contents or people shown?\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt1, image_landmark1, prompt2, image_landmark2, prompt3]\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.0,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9RdLpH128Ao"
   },
   "source": [
    "> You can confirm that the location is indeed Antalya, Turkey by visiting the Wikipedia page: https://en.wikipedia.org/wiki/Antalya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksjZiIfnO0zQ",
    "tags": []
   },
   "source": [
    "## Extracting tags of objects throughout the video\n",
    "\n",
    "Gemini can also extract tags throughout a video.\n",
    "\n",
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9qE2GGIA975",
    "outputId": "bbe9dcee-772e-4d8d-8d8c-6ee03606ea78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "- What is in the video?\n",
    "- What is the action in the video?\n",
    "- Provide 10 best tags for this video?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiQzvIJfQxbQ"
   },
   "source": [
    "## Asking more questions about a video\n",
    "\n",
    "Below is another example of using Gemini to ask questions the video and return a JSON response.\n",
    "\n",
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4  \n",
    "> _Note: Although this video contains audio, Gemini does not currently support audio input and will only answer based on the video._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQIV9SwCz5WM",
    "outputId": "646439d7-cae1-4788-e172-59d45afce1bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What is the profession of the main person?\n",
    "What are the main features of the phone highlighted?\n",
    "Which city was this recorded in?\n",
    "Provide the answer JSON.\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LurOmNuRpDr"
   },
   "source": [
    "## Retrieving extra information beyond the video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CY-zlixU87O",
    "outputId": "360ee28f-6f87-49df-f005-12b7ded61785",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Which line is this?\n",
    "where does it go?\n",
    "What are the stations/stops?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZrxMm_83Vps"
   },
   "source": [
    "> You can confirm that this is indeed the Confederation Line on Wikipedia here: https://en.wikipedia.org/wiki/Confederation_Line\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
