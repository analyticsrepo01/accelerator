{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Stable Diffusion V2.1\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_2_1.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_2_1.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_2_1.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "Open in Vertex AI Workbench\n",
    "    </a>\n",
    "    (a Python-3 GPU notebook with preinstalled HuggingFace/transformer libraries is recommended)\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates running local inference for [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) on either [Colab](https://colab.research.google.com) or [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench). This notebook also demonstrates finetuning stabilityai/stable-diffusion-2-1 with [Dreambooth](https://huggingface.co/docs/diffusers/training/dreambooth) and deploying it on Vertex AI for online prediction.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Run local predictions for text-to-image and text-guided-image-to-image with serving dockers.\n",
    "- Finetune the stabilityai/stable-diffusion-2-1 model with [Dreambooth](https://huggingface.co/docs/diffusers/training/dreambooth).\n",
    "- Upload the model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
    "- Deploy the model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
    "- Run online predictions for text-to-image and text-guided-image-to-image.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "**NOTE**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb671e75ca7b"
   },
   "source": [
    "#### Workbench only\n",
    "1.   Follow [this link](https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_2_1.ipynb) to deploy the notebook to a Vertex AI Workbench Instance.\n",
    "2.   Select `Create a new Notebook`.\n",
    "3.   Click `Advanced Options`.\n",
    "4.   In the **Environment** tab, select `Debian 10` for **Operating System** and select `Custom Container` for **Environment**.\n",
    "5.   Set `Docker container image` to `us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/transformers-notebook`.\n",
    "6.   Under **Machine configuration**, select 1 `T4` GPU and select `Install NVIDIA GPU driver automatically for me`.\n",
    "7.   Click `Create` to create the Vertex AI Workbench instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### Setup Google Cloud project\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
    "\n",
    "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
    "\n",
    "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "Fill following variables for experiments environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "855d6b96f291",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://my-project-0004-346516-asia-notebooks/...\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please enter your name/initials (no spaces or special characters allowed), ensure that it is unique\n",
    "UNIQUE_PREFIX=\"asia-notebooks\" ### PLEASE UPDATE THIS\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"asia-southeast1\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output. Fill it without the 'gs://' prefix.\n",
    "GCS_BUCKET = f\"{PROJECT_ID}-{UNIQUE_PREFIX}\"  # @param {type:\"string\"} \n",
    "BUCKET_URI = f\"gs://{GCS_BUCKET}\"  # @param {type:\"string\"}\n",
    "\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = !(gcloud config get-value core/account)  # @param {type:\"string\"}\n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "\n",
    "! gcloud storage buckets create {BUCKET_URI} --project={PROJECT_ID} --location={REGION}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "Initialize Vertex-AI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "12cd25839741",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==2.0.1+cu118\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.1.2)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.28.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1+cu118) (17.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1+cu118) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
      "Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: triton, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.0 requires torch==2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.0.1+cu118 triton-2.0.0\n",
      "Requirement already satisfied: transformers==4.35.0 in /opt/conda/lib/python3.10/site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers==4.35.0) (4.66.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.0) (2023.7.22)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: diffusers==0.22.3 in /opt/conda/lib/python3.10/site-packages (0.22.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (6.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (0.17.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (1.26.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (0.4.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers==0.22.3) (10.1.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.22.3) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.22.3) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.22.3) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.22.3) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.22.3) (23.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.22.3) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.22.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.22.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.22.3) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.22.3) (2023.7.22)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets==2.9.0 in /opt/conda/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/jupyter/.local/lib/python3.10/site-packages (from datasets==2.9.0) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets==2.9.0) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.9.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (0.17.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.9.0) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.9.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.9.0) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.9.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.9.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.9.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.9.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.9.0) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.9.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.9.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.9.0) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.9.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.9.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.9.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.9.0) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate==0.24.1 in /opt/conda/lib/python3.10/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1) (0.17.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.24.1) (3.28.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.24.1) (17.0.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.24.1) (1.3.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (0.4.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.10/site-packages (from gdown) (4.66.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==2.2.0\n",
      "  Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Collecting triton==2.2.0 (from torch==2.2.0)\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-aiplatform: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_aiplatform-1.42.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: triton, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1+cu118\n",
      "    Uninstalling torch-2.0.1+cu118:\n",
      "      Successfully uninstalled torch-2.0.1+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.2.0 triton-2.2.0\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)\n",
    "\n",
    "# Install Dependencies\n",
    "! pip3 install --upgrade pip\n",
    "! pip3 install torch==2.0.1+cu118  -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip3 install transformers==4.35.0\n",
    "! pip3 install diffusers==0.22.3\n",
    "! pip3 install datasets==2.9.0\n",
    "! pip3 install accelerate==0.24.1\n",
    "! pip3 install safetensors\n",
    "\n",
    "# Install gdown for downloading example training images.\n",
    "! pip3 install gdown\n",
    "\n",
    "!pip3 install torch==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b42bd4fa2b2d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The pre-built training docker image. It contains training scripts and models.\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-train:latest\"\n",
    "\n",
    "# The pre-built serving docker image. It contains serving scripts and models.\n",
    "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240223_1230_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f",
    "tags": []
   },
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "354da31189dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "from google.cloud import aiplatform, storage\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def create_job_name(prefix):\n",
    "    user = os.environ.get(\"USER\")\n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    job_name = f\"{prefix}-{user}-{now}\"\n",
    "    return job_name\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "def image_to_base64(image, format=\"JPEG\"):\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=format)\n",
    "    image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return image_str\n",
    "\n",
    "\n",
    "def base64_to_image(image_str):\n",
    "    image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
    "    return image\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows=2, cols=2):\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def deploy_model(model_id, task):\n",
    "    model_name = model_id\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-{task}-endpoint\")\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"TASK\": task,\n",
    "    }\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=\"g2-standard-8\",\n",
    "        accelerator_type=\"NVIDIA_L4\",\n",
    "        accelerator_count=1,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "def get_bucket_and_blob_name(filepath):\n",
    "    # The gcs path is of the form gs://<bucket-name>/<blob-name>\n",
    "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
    "    return tuple(gs_suffix.split(\"/\", 1))\n",
    "\n",
    "\n",
    "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
    "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    for local_file in glob.glob(local_dir_path + \"/**\"):\n",
    "        if not os.path.isfile(local_file):\n",
    "            continue\n",
    "        filename = local_file[1 + len(local_dir_path) :]\n",
    "        gcs_file_path = os.path.join(gcs_dir_path, filename)\n",
    "        _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.upload_from_filename(local_file)\n",
    "        print(\"Copied {} to {}.\".format(local_file, gcs_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e70e3519ff8b",
    "tags": []
   },
   "source": [
    "## Finetune with Dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dc65d8f0689"
   },
   "source": [
    "This section uses [dreambooth](https://dreambooth.github.io/) to finetune the [stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) model with [5 dog images](https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ) to personalize the text-to-image model.\n",
    "\n",
    "It finetunes both text encoder and unet of the stable diffusion model up to 800 steps. The whole finetuning job takes 45 minutes to finish using 1 A100 GPU.\n",
    "\n",
    "The full model will be saved after the finetuning job finishs and it can be loaded by the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img) to run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "34048707df5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Processing file 1aIWXIj3iB-7CfRFLROHq9U09EdrpUH-T alvan-nee-9M0tSjb-cpA-unsplash.jpeg\n",
      "Processing file 15IDLUW4Jpwot_xVpWWVyCONWpiBBpQIi alvan-nee-bQaAJCbNq3g-unsplash.jpeg\n",
      "Processing file 1FYFAp1sFfYh7I2eDcLK_BP7diKgGQib_ alvan-nee-brFsZ7qszSY-unsplash.jpeg\n",
      "Processing file 1zonwTmg7Ggl_w2ELQyTDVNkjtQTlsDq4 alvan-nee-eoqnr8ikwFE-unsplash.jpeg\n",
      "Processing file 1vI0QwqfGyWNdloy0L9N4Gz4qN3_BduwL alvan-nee-Id1DBHv4fbg-unsplash.jpeg\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1aIWXIj3iB-7CfRFLROHq9U09EdrpUH-T\n",
      "To: /home/jupyter/GenAI5/Course/dog/alvan-nee-9M0tSjb-cpA-unsplash.jpeg\n",
      "100%|█████████████████████████████████████████| 677k/677k [00:00<00:00, 108MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=15IDLUW4Jpwot_xVpWWVyCONWpiBBpQIi\n",
      "To: /home/jupyter/GenAI5/Course/dog/alvan-nee-bQaAJCbNq3g-unsplash.jpeg\n",
      "100%|███████████████████████████████████████| 1.40M/1.40M [00:00<00:00, 150MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FYFAp1sFfYh7I2eDcLK_BP7diKgGQib_\n",
      "To: /home/jupyter/GenAI5/Course/dog/alvan-nee-brFsZ7qszSY-unsplash.jpeg\n",
      "100%|███████████████████████████████████████| 1.19M/1.19M [00:00<00:00, 122MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zonwTmg7Ggl_w2ELQyTDVNkjtQTlsDq4\n",
      "To: /home/jupyter/GenAI5/Course/dog/alvan-nee-eoqnr8ikwFE-unsplash.jpeg\n",
      "100%|███████████████████████████████████████| 1.17M/1.17M [00:00<00:00, 147MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vI0QwqfGyWNdloy0L9N4Gz4qN3_BduwL\n",
      "To: /home/jupyter/GenAI5/Course/dog/alvan-nee-Id1DBHv4fbg-unsplash.jpeg\n",
      "100%|███████████████████████████████████████| 1.16M/1.16M [00:00<00:00, 105MB/s]\n",
      "Download completed\n",
      "Copied dog/alvan-nee-eoqnr8ikwFE-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog/alvan-nee-eoqnr8ikwFE-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-bQaAJCbNq3g-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog/alvan-nee-bQaAJCbNq3g-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-9M0tSjb-cpA-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog/alvan-nee-9M0tSjb-cpA-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-brFsZ7qszSY-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog/alvan-nee-brFsZ7qszSY-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-Id1DBHv4fbg-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog/alvan-nee-Id1DBHv4fbg-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-eoqnr8ikwFE-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog_class/alvan-nee-eoqnr8ikwFE-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-bQaAJCbNq3g-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog_class/alvan-nee-bQaAJCbNq3g-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-9M0tSjb-cpA-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog_class/alvan-nee-9M0tSjb-cpA-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-brFsZ7qszSY-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog_class/alvan-nee-brFsZ7qszSY-unsplash.jpeg.\n",
      "Copied dog/alvan-nee-Id1DBHv4fbg-unsplash.jpeg to gs://my-project-0004-346516-asia-notebooks/dreambooth/dog_class/alvan-nee-Id1DBHv4fbg-unsplash.jpeg.\n"
     ]
    }
   ],
   "source": [
    "# Download example training images.\n",
    "!gdown --folder https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ\n",
    "\n",
    "# Upload data to Cloud Storage bucket.\n",
    "upload_local_dir_to_gcs(\"dog\", f\"gs://{GCS_BUCKET}/dreambooth/dog\")\n",
    "upload_local_dir_to_gcs(\"dog\", f\"gs://{GCS_BUCKET}/dreambooth/dog_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "969cfeb79317"
   },
   "source": [
    "**NOTE**: If the upload step fails due to lacking of permission, you need to [grant the Storage Object Admin role](https://cloud.google.com/storage/docs/access-control/using-iam-permissions) for the Cloud account of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "65467b361315",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://my-project-0004-346516-asia-notebooks/aiplatform-custom-training-2024-03-06-14:26:30.814 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-southeast1/training/4740190890167894016?project=255766800726\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-southeast1/training/5815566038690955264?project=255766800726\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016\n",
      "Training did not produce a Managed Model returning None. Training Pipeline projects/255766800726/locations/asia-southeast1/trainingPipelines/4740190890167894016 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "# The pre-trained model to be loaded.\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "# Input and output path.\n",
    "instance_dir = f\"/gcs/{GCS_BUCKET}/dreambooth/dog\"\n",
    "class_dir = f\"/gcs/{GCS_BUCKET}/dreambooth/dog_class\"\n",
    "output_dir = f\"/gcs/{GCS_BUCKET}/dreambooth/output\"\n",
    "\n",
    "# Worker pool spec.\n",
    "machine_type = \"a2-highgpu-1g\"\n",
    "num_nodes = 1\n",
    "gpu_type = \"NVIDIA_TESLA_A100\"\n",
    "num_gpus = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = create_job_name(\"dreambooth-stable-diffusion\")\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "# See https://github.com/huggingface/diffusers/blob/v0.14.0/examples/dreambooth/train_dreambooth.py#L75\n",
    "# for a full list of training arguments.\n",
    "\n",
    "\n",
    "def train_model_with_config(num_nodes, machine_type, gpu_type, num_gpus ):\n",
    "\n",
    "    model = job.run(\n",
    "    args=[\n",
    "        \"dreambooth/train_dreambooth.py\",\n",
    "        f\"--pretrained_model_name_or_path={model_id}\",\n",
    "        \"--train_text_encoder\",\n",
    "        f\"--instance_data_dir={instance_dir}\",\n",
    "        f\"--class_data_dir={class_dir}\",\n",
    "        f\"--output_dir={output_dir}\",\n",
    "        \"--mixed_precision=fp16\",\n",
    "        \"--with_prior_preservation\",\n",
    "        \"--prior_loss_weight=1.0\",\n",
    "        \"--instance_prompt='a photo of sks dog'\",\n",
    "        \"--class_prompt='a photo of dog'\",\n",
    "        \"--resolution=512\",\n",
    "        \"--train_batch_size=1\",\n",
    "        \"--sample_batch_size=1\",\n",
    "        \"--gradient_accumulation_steps=1\",\n",
    "        \"--gradient_checkpointing\",\n",
    "        \"--learning_rate=5e-6\",\n",
    "        \"--lr_scheduler=constant\",\n",
    "        \"--lr_warmup_steps=0\",\n",
    "        \"--num_class_images=200\",\n",
    "        \"--max_train_steps=800\",\n",
    "    ],\n",
    "    replica_count=num_nodes,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=gpu_type,\n",
    "    accelerator_count=num_gpus,\n",
    "    )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Code that might potentially cause an error\n",
    "    machine_type = \"a2-highgpu-1g\"\n",
    "    num_nodes = 1\n",
    "    gpu_type = \"NVIDIA_TESLA_A100\"\n",
    "    num_gpus = 1\n",
    "    model = train_model_with_config(num_nodes, machine_type, gpu_type, num_gpus)\n",
    "except :  # Replace 'ErrorType' with the specific error you want to catch \n",
    "    try : \n",
    "        print(\"Error may becaused due to machine unavailable - A100s - trying L4s \")\n",
    "        machine_type = \"g2-standard-96\"\n",
    "        num_nodes = 1\n",
    "        gpu_type = \"NVIDIA_L4\"\n",
    "        num_gpus = 8\n",
    "        model = train_model_with_config(num_nodes, machine_type, gpu_type, num_gpus)\n",
    "    except :\n",
    "        print(\"Error may becaused due to machine unavailable - L4s also - trying A100s again \")\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        num_nodes = 1\n",
    "        gpu_type = \"NVIDIA_TESLA_A100\"\n",
    "        num_gpus = 1\n",
    "        model = train_model_with_config(num_nodes, machine_type, gpu_type, num_gpus)\n",
    "else:\n",
    "    # Code to execute if there's no error in the 'try' block\n",
    "    print(\"Error may becaused due to machine unavailable of any type \")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf7f82732e61",
    "tags": []
   },
   "source": [
    "## Upload and Deploy models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cc26e68d7b0"
   },
   "source": [
    "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
    "\n",
    "The model deployment step will take ~15 minutes to complete. \n",
    "\n",
    "The first request requires some additional time for model compilation (up to a\n",
    "few minutes), but the future requests should be processed much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd7b56421392"
   },
   "source": [
    "### Text-to-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d331b1ea337"
   },
   "source": [
    "Deploy the stable diffusion model for the `text-to-image` task.\n",
    "\n",
    "Once deployed, you can send a batch of text prompts to the endpoint to generated images.\n",
    "\n",
    "When deployed on one `NVIDIA_L4` GPU, the averaged inference time of a request is ~3-4 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf55e38815dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/255766800726/locations/asia-southeast1/endpoints/7249731072810811392/operations/581697726186520576\n",
      "Endpoint created. Resource name: projects/255766800726/locations/asia-southeast1/endpoints/7249731072810811392\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/255766800726/locations/asia-southeast1/endpoints/7249731072810811392')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/255766800726/locations/asia-southeast1/models/3339251997927800832/operations/4130534232554471424\n",
      "Model created. Resource name: projects/255766800726/locations/asia-southeast1/models/3339251997927800832@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/255766800726/locations/asia-southeast1/models/3339251997927800832@1')\n",
      "Deploying model to Endpoint : projects/255766800726/locations/asia-southeast1/endpoints/7249731072810811392\n",
      "Deploy Endpoint model backing LRO: projects/255766800726/locations/asia-southeast1/endpoints/7249731072810811392/operations/7652349141158199296\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Set the model_id to \"stabilityai/stable-diffusion-2-1\" to load the OSS pre-trained model.\n",
    "# model, endpoint = deploy_model(\n",
    "#     model_id=f\"gs://{GCS_BUCKET}/dreambooth/output\", task=\"text-to-image\"\n",
    "# )\n",
    "\n",
    "model, endpoint = deploy_model(\n",
    "    model_id=\"stabilityai/stable-diffusion-2-1\", task=\"text-to-image\"\n",
    ")\n",
    "\n",
    "time.sleep(660)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80b3fd2ace09"
   },
   "source": [
    "NOTE: The model weights will be downloaded after the deployment succeeds. Thus upto 10 minutes of additional waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ab04da3ec9a"
   },
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\"prompt\": \"a deer in photography style\"},\n",
    "    {\"prompt\": \"a sks dog in photography style\"},\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "images = [base64_to_image(image) for image in response.predictions]\n",
    "image_grid(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "Clean up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint.\n",
    "#endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "#model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1e51f764a60"
   },
   "source": [
    "### Text-guided image-to-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa686a54047c"
   },
   "source": [
    "Deploy the stable diffusion model for the text-guided image-to-image task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65e32356fbd1"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Set the model_id to a GCS path, like \"gs://GCS_BUCKET/dreambooth/output\", to load the dreambooth finetuned model above.\n",
    "model, endpoint = deploy_model(\n",
    "    model_id=\"stabilityai/stable-diffusion-2-1\", task=\"image-to-image\"\n",
    ")\n",
    "\n",
    "time.sleep(540)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsjYFLvNymc0"
   },
   "source": [
    "NOTE: The model weights will be downloaded after the deployment succeeds. Thus additional 5 minutes of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoints_ids = !(gcloud ai endpoints list --region='asia-southeast1')\n",
    "\n",
    "# endpoints_id = endpoints_ids[2]\n",
    "# print(endpoints_id)\n",
    "\n",
    "# ENDPOINT_DISPLAY_NAME = 'mixtral-serve-vllm_20240229_073940-endpoint'\n",
    "\n",
    "# ENDPOINT_ID=!(gcloud ai endpoints list \\\n",
    "#           --region='asia-southeast1' \\\n",
    "#           --filter=displayName:'mixtral-serve-vllm_20240229_073940-endpoint' \\\n",
    "#           --format=\"value(ENDPOINT_ID.scope())\")\n",
    "\n",
    "# print(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83a50fd4a1ed"
   },
   "outputs": [],
   "source": [
    "init_image = download_image(\n",
    "    \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    ")\n",
    "display(init_image)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"A fantasy landscape, trending on artstation photgraphic style\",\n",
    "        \"image\": image_to_base64(init_image),\n",
    "    },\n",
    "]\n",
    "\n",
    "# endpoint=ENDPOINT_ID\n",
    "\n",
    "response = endpoint.predict(instances=instances)\n",
    "images = [base64_to_image(image) for image in response.predictions]\n",
    "display(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed3795d474b9"
   },
   "source": [
    "Clean up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b53b883257b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint.\n",
    "#endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "#model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8a42fa49305",
    "tags": []
   },
   "source": [
    "## Run inferences locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d5ebc91c786",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Text-to-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d39ed8c97cc5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from diffusers import DPMSolverMultistepScheduler, StableDiffusionPipeline\n",
    "# from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#     model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "# )\n",
    "# pipe.to(device)\n",
    "\n",
    "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "#     pipe.scheduler.config, algorithm_type=\"dpmsolver++\"\n",
    "# )\n",
    "# pipe.unet.set_attn_processor(AttnProcessor2_0())\n",
    "# pipe.unet.to(memory_format=torch.channels_last)\n",
    "\n",
    "\n",
    "# prompt = \"A DSLR photo of Mt. Fuji with cherry blossom in the background\"\n",
    "\n",
    "# results = pipe(prompt=prompt, height=768, width=768, num_inference_steps=25)\n",
    "# images = results.images\n",
    "# display(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aed5ed7b6f6",
    "tags": []
   },
   "source": [
    "### Text-guided image-to-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faadabb7728f"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from diffusers import (DPMSolverMultistepScheduler,\n",
    "#                        StableDiffusionImg2ImgPipeline)\n",
    "# from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "# pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "#     model_id, torch_dtype=torch.float16\n",
    "# )\n",
    "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe = pipe.to(device)\n",
    "\n",
    "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "#     pipe.scheduler.config, algorithm_type=\"dpmsolver++\"\n",
    "# )\n",
    "# pipe.unet.set_attn_processor(AttnProcessor2_0())\n",
    "# pipe.unet.to(memory_format=torch.channels_last)\n",
    "\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "# init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "# init_image = init_image.resize((768, 512))\n",
    "\n",
    "# prompt = \"A fantasy landscape, trending on artstation\"\n",
    "\n",
    "# results = pipe(prompt=prompt, image=init_image)\n",
    "# images = results.images\n",
    "# display(init_image)\n",
    "# display(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_garden_pytorch_stable_diffusion_2_1.ipynb",
   "toc_visible": false
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
